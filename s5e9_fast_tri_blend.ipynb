{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e611af9c",
   "metadata": {},
   "source": [
    "# Playground Series S5E9 — Fast, Efficient Tri‑Blend (LGBM + XGB + CatBoost)\n",
    "\n",
    "This notebook implements a **head‑to‑tail, fast, and competitive baseline** for the Kaggle Playground Series S5E9 (Predicting the Beats‑per‑Minute of Songs).  \n",
    "It follows a proven recipe that’s been consistently strong on tabular regression: **5‑fold CV**, **LightGBM / XGBoost / CatBoost**, and a small **weighted blend** tuned on OOF predictions.\n",
    "\n",
    "**Highlights**\n",
    "- Minimal preprocessing (label‑encode object cols, median impute numeric).\n",
    "- Strong defaults with early stopping (fast) + 5‑fold CV (reliable).\n",
    "- Tiny, grid‑searched blend weights that usually beat any single model.\n",
    "- Produces **OOF CV metrics**, **feature importances**, and **submission.csv**.\n",
    "\n",
    "> Tip: If you want an extra ~0.005–0.01 RMSE, duplicate models with 2–3 different `SEEDS` and average per‑model before blending. This notebook starts with one seed for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9ee33",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00d8a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Tree models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor, Pool\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration & Imports\n",
    "# =========================\n",
    "import os, math, gc, warnings, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Tree models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Kaggle input path\n",
    "INPUT_DIR = Path(\"/kaggle/input/playground-series-s5e9\")\n",
    "assert INPUT_DIR.exists(), \"Expected Kaggle dataset path to exist: /kaggle/input/playground-series-s5e9\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Cross-validation\n",
    "N_SPLITS = 5\n",
    "SHUFFLE = True\n",
    "\n",
    "# Early stopping patience\n",
    "EARLY_STOPPING = 400\n",
    "\n",
    "# If you want to try seed ensembling later, add more seeds here and loop.\n",
    "SEEDS = [SEED]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15864516",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ee362a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load train / test / sample\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_path = \u001b[43mINPUT_DIR\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m test_path  = INPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m sub_path   = INPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33msample_submission.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'INPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Load train / test / sample\n",
    "# =========================\n",
    "train_path = INPUT_DIR / \"train.csv\"\n",
    "test_path  = INPUT_DIR / \"test.csv\"\n",
    "sub_path   = INPUT_DIR / \"sample_submission.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sub_path)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test  shape:\", test.shape)\n",
    "display(train.head(3))\n",
    "display(test.head(3))\n",
    "display(sample_submission.head(3))\n",
    "\n",
    "# Identify ID and TARGET\n",
    "# Most likely: id / bpm. We'll infer TARGET from sample_submission (2nd column).\n",
    "ID_COL = \"id\" if \"id\" in train.columns else train.columns[0]\n",
    "TARGET = sample_submission.columns[1] if len(sample_submission.columns) >= 2 else \"bpm\"\n",
    "assert TARGET in train.columns, f\"Could not find target '{TARGET}' in train columns {train.columns.tolist()}\"\n",
    "print(f\"Using ID_COL='{ID_COL}', TARGET='{TARGET}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce3bf3",
   "metadata": {},
   "source": [
    "## 3. Minimal Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Minimal preprocessing\n",
    "# - Label-encode object cols (fit on combined train+test)\n",
    "# - Fill numeric NAs with median; object NAs to 'NA'\n",
    "# =========================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Separate feature columns\n",
    "features = [c for c in train.columns if c not in [ID_COL, TARGET]]\n",
    "X = train[features].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# Detect dtypes\n",
    "obj_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Label-encode object columns using combined train+test categories\n",
    "for col in obj_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X[col], X_test[col]], axis=0).astype(str).fillna(\"NA\")\n",
    "    le.fit(combined)\n",
    "    X[col] = le.transform(X[col].astype(str).fillna(\"NA\"))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str).fillna(\"NA\"))\n",
    "\n",
    "# Fill numeric NAs with median\n",
    "for col in num_cols:\n",
    "    med = X[col].median()\n",
    "    X[col] = X[col].fillna(med)\n",
    "    X_test[col] = X_test[col].fillna(med)\n",
    "\n",
    "y = train[TARGET].values\n",
    "\n",
    "print(f\"Features: {len(features)} | Numeric: {len(num_cols)} | Object-encoded: {len(obj_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca4073",
   "metadata": {},
   "source": [
    "## 4. CV Helper & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2975ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2d0a9",
   "metadata": {},
   "source": [
    "## 5. Model Configurations (Regularized & Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Base parameters chosen for speed and stability.\n",
    "# Increase n_estimators if you disable early stopping.\n",
    "# =========================\n",
    "lgb_params = dict(\n",
    "    objective=\"regression\", metric=\"rmse\",\n",
    "    learning_rate=0.03, num_leaves=64, max_depth=-1,\n",
    "    min_data_in_leaf=40, feature_fraction=0.9, bagging_fraction=0.9,\n",
    "    bagging_freq=1, reg_lambda=2.0, reg_alpha=0.2, n_estimators=4000,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"reg:squarederror\", tree_method=\"hist\",\n",
    "    learning_rate=0.03, max_depth=7, min_child_weight=8,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_lambda=2.0, reg_alpha=0.2,\n",
    "    n_estimators=4000, random_state=SEED\n",
    ")\n",
    "\n",
    "cat_params = dict(\n",
    "    loss_function=\"RMSE\", depth=8, learning_rate=0.03,\n",
    "    l2_leaf_reg=6.0, iterations=4000, random_seed=SEED,\n",
    "    od_type=\"Iter\", od_wait=EARLY_STOPPING, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb377f7",
   "metadata": {},
   "source": [
    "## 6. Train with 5‑Fold CV & Collect OOF / Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers\n",
    "oof_lgb = np.zeros(len(X), dtype=float)\n",
    "oof_xgb = np.zeros(len(X), dtype=float)\n",
    "oof_cat = np.zeros(len(X), dtype=float)\n",
    "\n",
    "pred_lgb = np.zeros(len(X_test), dtype=float)\n",
    "pred_xgb = np.zeros(len(X_test), dtype=float)\n",
    "pred_cat = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "# For importances\n",
    "fi_lgb = np.zeros(len(features), dtype=float)\n",
    "fi_xgb = np.zeros(len(features), dtype=float)\n",
    "fi_cat = np.zeros(len(features), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgbm.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        callbacks=[lgb.early_stopping(EARLY_STOPPING), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    oof_lgb[va_idx] = lgbm.predict(X_va, num_iteration=lgbm.best_iteration_)\n",
    "    pred_lgb += lgbm.predict(X_test, num_iteration=lgbm.best_iteration_) / N_SPLITS\n",
    "    try:\n",
    "        fi_lgb += lgbm.feature_importances_ / N_SPLITS\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # XGBoost\n",
    "    xgbm = xgb.XGBRegressor(**xgb_params)\n",
    "    xgbm.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=EARLY_STOPPING\n",
    "    )\n",
    "    oof_xgb[va_idx] = xgbm.predict(X_va, iteration_range=(0, xgbm.best_iteration+1 if xgbm.best_iteration else 0))\n",
    "    pred_xgb += xgbm.predict(X_test, iteration_range=(0, xgbm.best_iteration+1 if xgbm.best_iteration else 0)) / N_SPLITS\n",
    "    try:\n",
    "        fi_xgb += xgbm.feature_importances_ / N_SPLITS\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # CatBoost\n",
    "    cat = CatBoostRegressor(**cat_params)\n",
    "    cat.fit(Pool(X_tr, y_tr), eval_set=Pool(X_va, y_va), use_best_model=True)\n",
    "    oof_cat[va_idx] = cat.predict(X_va)\n",
    "    pred_cat += cat.predict(X_test) / N_SPLITS\n",
    "    try:\n",
    "        fi_cat += cat.get_feature_importance(Pool(X_va, y_va)) / N_SPLITS\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# CV metrics\n",
    "rmse_lgb = rmse(y, oof_lgb)\n",
    "rmse_xgb = rmse(y, oof_xgb)\n",
    "rmse_cat = rmse(y, oof_cat)\n",
    "\n",
    "print(\"\\nOOF RMSEs:\")\n",
    "print(f\"  LightGBM : {rmse_lgb:.6f}\")\n",
    "print(f\"  XGBoost  : {rmse_xgb:.6f}\")\n",
    "print(f\"  CatBoost : {rmse_cat:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b0ee2",
   "metadata": {},
   "source": [
    "## 7. Simple Weighted Blend (Grid Search on OOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search weights (w1, w2, w3) for (LGB, XGB, CAT) s.t. w1 + w2 + w3 = 1\n",
    "grid = np.linspace(0.0, 1.0, 21)  # 0.05 steps\n",
    "best_rmse, best_w = 1e9, (1/3, 1/3, 1/3)\n",
    "\n",
    "for w1 in grid:\n",
    "    for w2 in grid:\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        if w3 < 0 or w3 > 1: \n",
    "            continue\n",
    "        oof_blend = w1 * oof_lgb + w2 * oof_xgb + w3 * oof_cat\n",
    "        score = rmse(y, oof_blend)\n",
    "        if score < best_rmse:\n",
    "            best_rmse, best_w = score, (w1, w2, w3)\n",
    "\n",
    "print(f\"Best blend OOF RMSE: {best_rmse:.6f}  |  weights (LGB, XGB, CAT) = {best_w}\")\n",
    "w1, w2, w3 = best_w\n",
    "\n",
    "# Final predictions\n",
    "pred_blend = w1 * pred_lgb + w2 * pred_xgb + w3 * pred_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29491375",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db218017",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission[submission.columns[1]] = pred_blend  # set target column values\n",
    "submission_path = OUTPUT_DIR / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d063690",
   "metadata": {},
   "source": [
    "## 9. Feature Importances (Average Across Folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919df1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average normalized importances from the three models (when available)\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"lgb\": fi_lgb,\n",
    "    \"xgb\": fi_xgb,\n",
    "    \"cat\": fi_cat\n",
    "})\n",
    "\n",
    "# Normalize each column (avoid divide by zero)\n",
    "for col in [\"lgb\", \"xgb\", \"cat\"]:\n",
    "    s = importances[col].sum()\n",
    "    importances[col] = importances[col] / s if s > 0 else importances[col]\n",
    "\n",
    "importances[\"avg\"] = importances[[\"lgb\", \"xgb\", \"cat\"]].mean(axis=1)\n",
    "imp_top = importances.sort_values(\"avg\", ascending=False).head(30)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.barh(imp_top[\"feature\"][::-1], imp_top[\"avg\"][::-1])\n",
    "plt.title(\"Top 30 Features (avg importance)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bed9dc",
   "metadata": {},
   "source": [
    "## 10. OOF Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7feaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df = pd.DataFrame({\n",
    "    ID_COL: train[ID_COL],\n",
    "    TARGET: y,\n",
    "    \"oof_lgb\": oof_lgb,\n",
    "    \"oof_xgb\": oof_xgb,\n",
    "    \"oof_cat\": oof_cat,\n",
    "    \"oof_blend\": w1*oof_lgb + w2*oof_xgb + w3*oof_cat,\n",
    "})\n",
    "oof_df[\"resid\"] = oof_df[\"oof_blend\"] - oof_df[TARGET]\n",
    "display(oof_df.head())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(oof_df[\"resid\"], bins=50)\n",
    "plt.title(\"OOF Residuals (Blend)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09969d",
   "metadata": {},
   "source": [
    "## 11. Next Steps (Optional)\n",
    "\n",
    "- **Seed diversity**: set `SEEDS = [42, 1337, 2025]` and average per‑model across seeds before blending.\n",
    "- **Light FE**: Try a few pairwise interactions among the top features (add `<f_i> * <f_j>`), but keep extras under ~30.\n",
    "- **Tune grid granularity**: tighten the blend search around the best weights with finer steps (e.g., 0.01) once CV stabilizes.\n",
    "- **Submission strategy**: keep submissions to a few strong variants validated by CV; avoid leaderboard overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
